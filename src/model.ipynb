{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pseudo-bulk bigwig file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools index ../data/initial_10x_outputs/atac_peaks/D20_1_ATAC.bam ../data/initial_10x_outputs/atac_peaks/D20_1_ATAC.bam.bai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bamFilesList: ['../results/bam_cell_type/D20_1/Myogenic.bam']\n",
      "binLength: 50\n",
      "numberOfSamples: None\n",
      "blackListFileName: None\n",
      "skipZeroOverZero: False\n",
      "bed_and_bin: False\n",
      "genomeChunkSize: None\n",
      "defaultFragmentLength: read length\n",
      "numberOfProcessors: 4\n",
      "verbose: False\n",
      "region: None\n",
      "bedFile: None\n",
      "minMappingQuality: None\n",
      "ignoreDuplicates: False\n",
      "chrsToSkip: []\n",
      "stepSize: 50\n",
      "center_read: False\n",
      "samFlag_include: None\n",
      "samFlag_exclude: None\n",
      "minFragmentLength: 0\n",
      "maxFragmentLength: 0\n",
      "zerosToNans: False\n",
      "smoothLength: None\n",
      "save_data: False\n",
      "out_file_for_raw_data: None\n",
      "maxPairedFragmentLength: 1000\n",
      "[W::bam_hdr_read] EOF marker is absent. The input is probably truncated\n",
      "[E::bgzf_read_block] Failed to read BGZF block data at offset 12081241563 expected 25580 bytes; hread returned 22386\n",
      "[E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes\n",
      "samtools index: failed to create index for \"../results/bam_cell_type/D20_1/Mesenchymal.bam\": No such file or directory\n",
      "The file '../results/bam_cell_type/D20_1/Mesenchymal.bam' does not exist\n",
      "bamFilesList: ['../results/bam_cell_type/D20_1/Neuroblast.bam']\n",
      "binLength: 50\n",
      "numberOfSamples: None\n",
      "blackListFileName: None\n",
      "skipZeroOverZero: False\n",
      "bed_and_bin: False\n",
      "genomeChunkSize: None\n",
      "defaultFragmentLength: read length\n",
      "numberOfProcessors: 4\n",
      "verbose: False\n",
      "region: None\n",
      "bedFile: None\n",
      "minMappingQuality: None\n",
      "ignoreDuplicates: False\n",
      "chrsToSkip: []\n",
      "stepSize: 50\n",
      "center_read: False\n",
      "samFlag_include: None\n",
      "samFlag_exclude: None\n",
      "minFragmentLength: 0\n",
      "maxFragmentLength: 0\n",
      "zerosToNans: False\n",
      "smoothLength: None\n",
      "save_data: False\n",
      "out_file_for_raw_data: None\n",
      "maxPairedFragmentLength: 1000\n",
      "bamFilesList: ['../results/bam_cell_type/D20_1/Somite.bam']\n",
      "binLength: 50\n",
      "numberOfSamples: None\n",
      "blackListFileName: None\n",
      "skipZeroOverZero: False\n",
      "bed_and_bin: False\n",
      "genomeChunkSize: None\n",
      "defaultFragmentLength: read length\n",
      "numberOfProcessors: 4\n",
      "verbose: False\n",
      "region: None\n",
      "bedFile: None\n",
      "minMappingQuality: None\n",
      "ignoreDuplicates: False\n",
      "chrsToSkip: []\n",
      "stepSize: 50\n",
      "center_read: False\n",
      "samFlag_include: None\n",
      "samFlag_exclude: None\n",
      "minFragmentLength: 0\n",
      "maxFragmentLength: 0\n",
      "zerosToNans: False\n",
      "smoothLength: None\n",
      "save_data: False\n",
      "out_file_for_raw_data: None\n",
      "maxPairedFragmentLength: 1000\n",
      "bamFilesList: ['../results/bam_cell_type/D20_1/Myoblast.bam']\n",
      "binLength: 50\n",
      "numberOfSamples: None\n",
      "blackListFileName: None\n",
      "skipZeroOverZero: False\n",
      "bed_and_bin: False\n",
      "genomeChunkSize: None\n",
      "defaultFragmentLength: read length\n",
      "numberOfProcessors: 4\n",
      "verbose: False\n",
      "region: None\n",
      "bedFile: None\n",
      "minMappingQuality: None\n",
      "ignoreDuplicates: False\n",
      "chrsToSkip: []\n",
      "stepSize: 50\n",
      "center_read: False\n",
      "samFlag_include: None\n",
      "samFlag_exclude: None\n",
      "minFragmentLength: 0\n",
      "maxFragmentLength: 0\n",
      "zerosToNans: False\n",
      "smoothLength: None\n",
      "save_data: False\n",
      "out_file_for_raw_data: None\n",
      "maxPairedFragmentLength: 1000\n",
      "bamFilesList: ['../results/bam_cell_type/D20_1/14.bam']\n",
      "binLength: 50\n",
      "numberOfSamples: None\n",
      "blackListFileName: None\n",
      "skipZeroOverZero: False\n",
      "bed_and_bin: False\n",
      "genomeChunkSize: None\n",
      "defaultFragmentLength: read length\n",
      "numberOfProcessors: 4\n",
      "verbose: False\n",
      "region: None\n",
      "bedFile: None\n",
      "minMappingQuality: None\n",
      "ignoreDuplicates: False\n",
      "chrsToSkip: []\n",
      "stepSize: 50\n",
      "center_read: False\n",
      "samFlag_include: None\n",
      "samFlag_exclude: None\n",
      "minFragmentLength: 0\n",
      "maxFragmentLength: 0\n",
      "zerosToNans: False\n",
      "smoothLength: None\n",
      "save_data: False\n",
      "out_file_for_raw_data: None\n",
      "maxPairedFragmentLength: 1000\n",
      "bamFilesList: ['../results/bam_cell_type/D20_1/Neuronal.bam']\n",
      "binLength: 50\n",
      "numberOfSamples: None\n",
      "blackListFileName: None\n",
      "skipZeroOverZero: False\n",
      "bed_and_bin: False\n",
      "genomeChunkSize: None\n",
      "defaultFragmentLength: read length\n",
      "numberOfProcessors: 4\n",
      "verbose: False\n",
      "region: None\n",
      "bedFile: None\n",
      "minMappingQuality: None\n",
      "ignoreDuplicates: False\n",
      "chrsToSkip: []\n",
      "stepSize: 50\n",
      "center_read: False\n",
      "samFlag_include: None\n",
      "samFlag_exclude: None\n",
      "minFragmentLength: 0\n",
      "maxFragmentLength: 0\n",
      "zerosToNans: False\n",
      "smoothLength: None\n",
      "save_data: False\n",
      "out_file_for_raw_data: None\n",
      "maxPairedFragmentLength: 1000\n",
      "^C\n",
      "Process SpawnPoolWorker-4:\n",
      "Process SpawnPoolWorker-2:\n",
      "Process SpawnPoolWorker-3:\n",
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/bin/bamCoverage\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/bamCoverage.py\", line 256, in main\n",
      "    wr.run(writeBedGraph.scaleCoverage, func_args, args.outFileName,\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 145, in run\n",
      "    res = mapReduce.mapReduce([func_to_call, func_args],\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/mapReduce.py\", line 142, in mapReduce\n",
      "    res = pool.map_async(func, TASKS).get(9999999)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 768, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 765, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/threading.py\", line 629, in wait\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 27, in writeBedGraph_wrapper\n",
      "    return WriteBedGraph.writeBedGraph_worker(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 269, in writeBedGraph_worker\n",
      "    if not np.isnan(previous_value):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 27, in writeBedGraph_wrapper\n",
      "    return WriteBedGraph.writeBedGraph_worker(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 231, in writeBedGraph_worker\n",
      "    coverage, _ = self.count_reads_in_region(chrom, start, end)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 501, in count_reads_in_region\n",
      "    tcov = self.get_coverage_of_region(bam, chrom, trans)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 697, in get_coverage_of_region\n",
      "    sIdx = vector_start + max((fragmentStart - reg[0]) // tileSize, 0)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 27, in writeBedGraph_wrapper\n",
      "    return WriteBedGraph.writeBedGraph_worker(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 231, in writeBedGraph_worker\n",
      "    coverage, _ = self.count_reads_in_region(chrom, start, end)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 501, in count_reads_in_region\n",
      "    tcov = self.get_coverage_of_ region(bam, chrom, trans)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 674, in get_coverage_of_region\n",
      "    position_blocks = fragmentFromRead_func(read)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "       File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 788, in get_fragment_from_read\n",
      "    def get_fragment_from_read(self, read):\n",
      "    \n",
      "^^^^^^^^^^^KeyboardInterrupt\n",
      "^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/threading.py\", line 331, in wait\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 27, in writeBedGraph_wrapper\n",
      "    return WriteBedGraph.writeBedGraph_worker(*args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/writeBedGraph.py\", line 231, in writeBedGraph_worker\n",
      "    coverage, _ = self.count_reads_in_region(chrom, start, end)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 501, in count_reads_in_region\n",
      "    tcov = self.get_coverage_of_region(bam, chrom, trans)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/deeptools/countReadsPerBin.py\", line 697, in get_coverage_of_region\n",
      "    sIdx = vector_start + max((fragmentStart - reg[0]) // tileSize, 0)\n",
      "                          ^^^^^^^^^^^^^    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/adorman/Documents/master-thesis/src/create_bw_cell_type.py\", line 50, in <module>\n",
      "    subprocess.run(bamCoverage_command, shell=True)\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/subprocess.py\", line 550, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/subprocess.py\", line 1201, in communicate\n",
      "    self.wait()\n"
     ]
    }
   ],
   "source": [
    "!python create_bw_cell_type.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader for discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = anndata.read_h5ad('../results/concat.h5ad')\n",
    "\n",
    "adata = pseudo_bulk(adata,'cell_type')\n",
    "fetch_sequence(adata, path_genome='../data/hg38.fa')\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model class + functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BPNet(nn.Module):\n",
    "    def __init__(self, nb_conv=8, nb_filters=64, first_kernel=21, rest_kernel=3, profile_kernel_size=75, out_pred_len=1000):\n",
    "        super().__init__()\n",
    "        \"\"\" BPNet architechture as in paper \n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        nb_conv: int (default 8)\n",
    "            number of convolutional layers\n",
    "\n",
    "        nb_filters: int (default 64)\n",
    "            number of filters in the convolutional layers\n",
    "\n",
    "        first_kernel: int (default 25)\n",
    "            size of the kernel in the first convolutional layer\n",
    "\n",
    "        rest_kernel: int (default 3)\n",
    "            size of the kernel in all convolutional layers except the first one\n",
    "\n",
    "        profile_kernel_size: int (default 75)\n",
    "            size of the kernel in the profile convolution\n",
    "\n",
    "        out_pred_len: int (default 1000)\n",
    "            number of bp for which ATAC signal is predicted\n",
    "\n",
    "        Model Architecture \n",
    "        ------------------------\n",
    "\n",
    "        - Body: sequence of convolutional layers with residual skip connections, dilated convolutions, \n",
    "        and  ReLU activation functions\n",
    "\n",
    "        - Head: \n",
    "            > Profile prediction head: a multinomial probability of Tn5 insertion counts at each position \n",
    "            in the input sequence, deconvolution layer\n",
    "            > Total count prediction: the total Tn5 insertion counts over the input region, global average\n",
    "            poooling and linear layer predicting the total count per strand\n",
    "        \n",
    "        The predicted (expected) count at a specific position is a multiplication of the predicted total \n",
    "        counts and the multinomial probability at that position.\n",
    "\n",
    "        -------------------------\n",
    "        \n",
    "        Reference: Avsec, Ž., Weilert, M., Shrikumar, A. et al. Base-resolution models of transcription-factor binding \n",
    "        reveal soft motif syntax. Nat Genet 53, 354–366 (2021). https://doi.org/10.1038/s41588-021-00782-6\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        #Define parameters\n",
    "        self.nb_conv = nb_conv\n",
    "        self.nb_filters = nb_filters\n",
    "        self.first_kernel = first_kernel\n",
    "        self.rest_kernel = rest_kernel\n",
    "        self.profile_kernel = profile_kernel_size\n",
    "        self.out_pred_len = out_pred_len\n",
    "\n",
    "        #Convolutional layers\n",
    "        self.convlayers = nn.ModuleList()\n",
    "\n",
    "        self.convlayers.append(nn.Conv1d(in_channels=4, \n",
    "                                         out_channels=self.nb_filters,\n",
    "                                         kernel_size=self.first_kernel))\n",
    "        for i in range (1,self.nb_conv):\n",
    "            self.convlayers.append(nn.Conv1d(in_channels=self.nb_filters, \n",
    "                                         out_channels=self.nb_filters,\n",
    "                                         kernel_size=self.rest_kernel,\n",
    "                                         dilation=2**i))\n",
    "        #Profile prediction head   \n",
    "        self.profile_conv = nn.ConvTranspose1d(self.nb_filters, 1, kernel_size=self.profile_kernel)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #Total count prediction head\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = nn.Linear(self.nb_filters,1)\n",
    "\n",
    "            \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #Residual + Dilated convolution layers\n",
    "        #-----------------------------------------------\n",
    "        x = F.relu(self.convlayers[0](x))\n",
    "\n",
    "        for layer in self.convlayers[1:]:\n",
    "            \n",
    "            conv_x = F.relu(layer(x))\n",
    "\n",
    "            #Crop output previous layer to size of current \n",
    "            x_len = x.size(2); conv_x_len = conv_x.size(2)\n",
    "            cropsize = (x_len - conv_x_len) // 2\n",
    "            x = x[:, :, cropsize:-cropsize] \n",
    "\n",
    "            #Skipped connection\n",
    "            x = conv_x + x    \n",
    "\n",
    "        #Profile head\n",
    "        #-----------------------------------------------\n",
    "        profile = self.profile_conv(x)\n",
    "        \n",
    "        cropsize = int((profile.size(2)/2) - (self.out_pred_len/2))\n",
    "        profile = profile[:,:, cropsize:-cropsize]\n",
    "        \n",
    "        profile = self.flatten(profile)\n",
    "\n",
    "        #Total count head\n",
    "        #-----------------------------------------------\n",
    "        count = self.global_pool(x)  \n",
    "        count = count.squeeze()\n",
    "        count = self.linear(count)\n",
    "\n",
    "        return x, profile, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the base-resolution 1,000 bp length Tn5 insertion count profile using two complementary outputs: (1) the total Tn5 insertion counts over the 1,000 bp region, and (2) a multinomial probability of Tn5 insertion counts at each position in the 1,000 bp sequence. The predicted (expected) count at a specific position is a multiplication of the predicted total counts and the multinomial probability at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9177e+00, -4.1933e+00, -4.1938e+00,  ..., -6.8152e+00,\n",
       "         -2.7607e+00, -1.0761e+01],\n",
       "        [-8.8051e+00, -9.7412e-01, -5.7450e+00,  ..., -5.6425e+00,\n",
       "         -3.1305e+00, -5.7297e+00],\n",
       "        [-3.5067e+00, -4.0354e+00, -9.2691e+00,  ..., -6.7895e+00,\n",
       "         -1.0056e+01, -4.8932e+00],\n",
       "        ...,\n",
       "        [-2.6666e+00, -1.8417e+00, -6.3831e+00,  ..., -7.5926e+00,\n",
       "         -7.9752e+00, -3.2860e+00],\n",
       "        [-5.9738e+00, -1.1024e+01, -6.0759e+00,  ..., -7.0113e+00,\n",
       "         -1.0954e+01, -4.0736e+00],\n",
       "        [ 2.6391e-03, -2.6899e+00, -4.6821e+00,  ..., -6.7192e+00,\n",
       "         -1.1125e+00, -7.1009e+00]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BPNet()\n",
    "input = torch.randn(32, 4, 2114)\n",
    "x, profile, count = m(input)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(16, 33, kernel_size=(3,), stride=(2,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv1d(16, 33, 3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BPNet(\n",
       "  (convlayers): ModuleList(\n",
       "    (0): Conv1d(4, 64, kernel_size=(25,), stride=(1,))\n",
       "    (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "    (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(8,))\n",
       "    (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(16,))\n",
       "    (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(32,))\n",
       "    (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(64,))\n",
       "    (7): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(128,))\n",
       "    (8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(256,))\n",
       "    (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), dilation=(512,))\n",
       "  )\n",
       "  (deconv): ConvTranspose1d(64, 2, kernel_size=(25,), stride=(1,), padding=(12,))\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeKIra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
