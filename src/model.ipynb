{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X:153066000-153070096     [0.016203300704057205, 0.016203300704057205, 0...\n",
       "14:105454000-105458096    [0.440591878718832, 0.4400747521006175, 0.4395...\n",
       "10:79848000-79852096      [0.09515129775148486, 0.0968750531455335, 0.09...\n",
       "7:158174000-158178096     [0.3611267550531898, 0.35992012627735576, 0.35...\n",
       "11:6454000-6458096        [0.9330687947985281, 0.9325516681803134, 0.932...\n",
       "                                                ...                        \n",
       "1:122534000-122538096     [0.05205741290026889, 0.052229788439673755, 0....\n",
       "4:87346000-87350096       [0.24546276811252615, 0.24529039257312127, 0.2...\n",
       "1:116528000-116532096     [0.3307886601179338, 0.3319952888937678, 0.333...\n",
       "11:71156000-71160096      [0.33440854644543594, 0.33509804860305537, 0.3...\n",
       "10:119150000-119154096    [0.6298602209853725, 0.6339972339310893, 0.638...\n",
       "Length: 289162, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_pickle('../results/Somite.pkl')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13080776"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test.tolist(), index= test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = test.apply(pd.Series)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X:153066000-153070096</th>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14:105454000-105458096</th>\n",
       "      <td>0.440592</td>\n",
       "      <td>0.440075</td>\n",
       "      <td>0.439558</td>\n",
       "      <td>0.439558</td>\n",
       "      <td>0.439385</td>\n",
       "      <td>0.439213</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.438868</td>\n",
       "      <td>0.438696</td>\n",
       "      <td>0.438351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037750</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.029476</td>\n",
       "      <td>0.025339</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.016720</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.008274</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10:79848000-79852096</th>\n",
       "      <td>0.095151</td>\n",
       "      <td>0.096875</td>\n",
       "      <td>0.098771</td>\n",
       "      <td>0.100667</td>\n",
       "      <td>0.102563</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>0.108252</td>\n",
       "      <td>0.110148</td>\n",
       "      <td>0.112044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7:158174000-158178096</th>\n",
       "      <td>0.361127</td>\n",
       "      <td>0.359920</td>\n",
       "      <td>0.358541</td>\n",
       "      <td>0.357162</td>\n",
       "      <td>0.355783</td>\n",
       "      <td>0.354404</td>\n",
       "      <td>0.353025</td>\n",
       "      <td>0.351646</td>\n",
       "      <td>0.350439</td>\n",
       "      <td>0.349233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.015859</td>\n",
       "      <td>0.013618</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11:6454000-6458096</th>\n",
       "      <td>0.933069</td>\n",
       "      <td>0.932552</td>\n",
       "      <td>0.932035</td>\n",
       "      <td>0.931690</td>\n",
       "      <td>0.931690</td>\n",
       "      <td>0.931862</td>\n",
       "      <td>0.932207</td>\n",
       "      <td>0.932552</td>\n",
       "      <td>0.932896</td>\n",
       "      <td>0.933414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>0.052747</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.040336</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.020168</td>\n",
       "      <td>0.013445</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1:122534000-122538096</th>\n",
       "      <td>0.052057</td>\n",
       "      <td>0.052230</td>\n",
       "      <td>0.052402</td>\n",
       "      <td>0.052575</td>\n",
       "      <td>0.052747</td>\n",
       "      <td>0.052919</td>\n",
       "      <td>0.053092</td>\n",
       "      <td>0.053264</td>\n",
       "      <td>0.053436</td>\n",
       "      <td>0.053609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4:87346000-87350096</th>\n",
       "      <td>0.245463</td>\n",
       "      <td>0.245290</td>\n",
       "      <td>0.245118</td>\n",
       "      <td>0.245118</td>\n",
       "      <td>0.245118</td>\n",
       "      <td>0.245290</td>\n",
       "      <td>0.245290</td>\n",
       "      <td>0.245118</td>\n",
       "      <td>0.244946</td>\n",
       "      <td>0.244773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1:116528000-116532096</th>\n",
       "      <td>0.330789</td>\n",
       "      <td>0.331995</td>\n",
       "      <td>0.333202</td>\n",
       "      <td>0.334409</td>\n",
       "      <td>0.335615</td>\n",
       "      <td>0.336822</td>\n",
       "      <td>0.338201</td>\n",
       "      <td>0.339752</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.342855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.012928</td>\n",
       "      <td>0.010343</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.005171</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11:71156000-71160096</th>\n",
       "      <td>0.334409</td>\n",
       "      <td>0.335098</td>\n",
       "      <td>0.335788</td>\n",
       "      <td>0.336477</td>\n",
       "      <td>0.337167</td>\n",
       "      <td>0.337856</td>\n",
       "      <td>0.338546</td>\n",
       "      <td>0.338890</td>\n",
       "      <td>0.339407</td>\n",
       "      <td>0.340269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.010343</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.005171</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10:119150000-119154096</th>\n",
       "      <td>0.629860</td>\n",
       "      <td>0.633997</td>\n",
       "      <td>0.638134</td>\n",
       "      <td>0.642099</td>\n",
       "      <td>0.646064</td>\n",
       "      <td>0.650028</td>\n",
       "      <td>0.653993</td>\n",
       "      <td>0.657957</td>\n",
       "      <td>0.661922</td>\n",
       "      <td>0.665542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>0.024822</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289162 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1         2         3         4     \\\n",
       "X:153066000-153070096   0.016203  0.016203  0.016203  0.016203  0.016203   \n",
       "14:105454000-105458096  0.440592  0.440075  0.439558  0.439558  0.439385   \n",
       "10:79848000-79852096    0.095151  0.096875  0.098771  0.100667  0.102563   \n",
       "7:158174000-158178096   0.361127  0.359920  0.358541  0.357162  0.355783   \n",
       "11:6454000-6458096      0.933069  0.932552  0.932035  0.931690  0.931690   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "1:122534000-122538096   0.052057  0.052230  0.052402  0.052575  0.052747   \n",
       "4:87346000-87350096     0.245463  0.245290  0.245118  0.245118  0.245118   \n",
       "1:116528000-116532096   0.330789  0.331995  0.333202  0.334409  0.335615   \n",
       "11:71156000-71160096    0.334409  0.335098  0.335788  0.336477  0.337167   \n",
       "10:119150000-119154096  0.629860  0.633997  0.638134  0.642099  0.646064   \n",
       "\n",
       "                            5         6         7         8         9     ...  \\\n",
       "X:153066000-153070096   0.016203  0.016203  0.016203  0.016203  0.016203  ...   \n",
       "14:105454000-105458096  0.439213  0.439040  0.438868  0.438696  0.438351  ...   \n",
       "10:79848000-79852096    0.104460  0.106356  0.108252  0.110148  0.112044  ...   \n",
       "7:158174000-158178096   0.354404  0.353025  0.351646  0.350439  0.349233  ...   \n",
       "11:6454000-6458096      0.931862  0.932207  0.932552  0.932896  0.933414  ...   \n",
       "...                          ...       ...       ...       ...       ...  ...   \n",
       "1:122534000-122538096   0.052919  0.053092  0.053264  0.053436  0.053609  ...   \n",
       "4:87346000-87350096     0.245290  0.245290  0.245118  0.244946  0.244773  ...   \n",
       "1:116528000-116532096   0.336822  0.338201  0.339752  0.341304  0.342855  ...   \n",
       "11:71156000-71160096    0.337856  0.338546  0.338890  0.339407  0.340269  ...   \n",
       "10:119150000-119154096  0.650028  0.653993  0.657957  0.661922  0.665542  ...   \n",
       "\n",
       "                            1015      1016      1017      1018      1019  \\\n",
       "X:153066000-153070096   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14:105454000-105458096  0.037750  0.033613  0.029476  0.025339  0.021030   \n",
       "10:79848000-79852096    0.021030  0.018617  0.016203  0.013790  0.011377   \n",
       "7:158174000-158178096   0.020513  0.018099  0.015859  0.013618  0.011377   \n",
       "11:6454000-6458096      0.058435  0.052747  0.047059  0.040336  0.033613   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "1:122534000-122538096   0.006206  0.005516  0.004827  0.004137  0.003448   \n",
       "4:87346000-87350096     0.006206  0.005516  0.004827  0.004137  0.003448   \n",
       "1:116528000-116532096   0.022926  0.020513  0.018099  0.015514  0.012928   \n",
       "11:71156000-71160096    0.015514  0.013790  0.012066  0.010343  0.008619   \n",
       "10:119150000-119154096  0.027925  0.024822  0.021719  0.018617  0.015514   \n",
       "\n",
       "                            1020      1021      1022      1023  1024  \n",
       "X:153066000-153070096   0.000000  0.000000  0.000000  0.000000   0.0  \n",
       "14:105454000-105458096  0.016720  0.012411  0.008274  0.004137   0.0  \n",
       "10:79848000-79852096    0.008964  0.006723  0.004482  0.002241   0.0  \n",
       "7:158174000-158178096   0.009136  0.006895  0.004654  0.002413   0.0  \n",
       "11:6454000-6458096      0.026891  0.020168  0.013445  0.006723   0.0  \n",
       "...                          ...       ...       ...       ...   ...  \n",
       "1:122534000-122538096   0.002758  0.002069  0.001379  0.000690   0.0  \n",
       "4:87346000-87350096     0.002758  0.002069  0.001379  0.000690   0.0  \n",
       "1:116528000-116532096   0.010343  0.007757  0.005171  0.002586   0.0  \n",
       "11:71156000-71160096    0.006895  0.005171  0.003448  0.001724   0.0  \n",
       "10:119150000-119154096  0.012411  0.009308  0.006206  0.003103   0.0  \n",
       "\n",
       "[289162 rows x 1025 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index    10767480\n",
       "0         2313296\n",
       "1         2313296\n",
       "2         2313296\n",
       "3         2313296\n",
       "           ...   \n",
       "1020      2313296\n",
       "1021      2313296\n",
       "1022      2313296\n",
       "1023      2313296\n",
       "1024      2313296\n",
       "Length: 1026, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATAC = pd.concat(pd.read_pickle(f) for f in ['../results/ATAC/D8/Somite.pkl', '../results/ATAC/D22-15/Myoblast.pkl'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#Create test dataset\n",
    "ATAC = pd.concat(pd.read_pickle(f) for f in ['../results/Somite.pkl', '../results/Myoblast.pkl'])\n",
    "with open('../results/ATAC_peaks.pkl', 'wb') as file:\n",
    "            pickle.dump(ATAC, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_datasets import PeaksDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = '../results/'\n",
    "chrom_train = ['1','2','3','4','5','7','8','9','10','11','12','14','15','16','17','18','19','20','21','X','Y']\n",
    "chrom_test = ['6']\n",
    "\n",
    "nb_back = 100\n",
    "pseudo_bulk_order = [\"D8Somite\",\"D22-15Myoblast\"]\n",
    "\n",
    "train_dataset = PeaksDataset(data_dir + 'peaks_seq.pkl', data_dir + 'background_GC_matched.pkl',\n",
    "                                 data_dir + 'ATAC_peakst.pkl', data_dir + 'ATAC_backgroundt.pkl', \n",
    "                                 chrom_test, pseudo_bulk_order, nb_back)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10dc99fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for i, data in tqdm(enumerate(train_dataloader)):\n",
    "    inputs, tracks = data \n",
    "    print(inputs)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models import BPNet\n",
    "\n",
    "m = BPNet()\n",
    "input = torch.randint(low=0, high=2, size=(32, 4, 4096)).float()\n",
    "tracks = torch.abs(torch.randn(size=(32,1025)))\n",
    "print(input.shape)\n",
    "x, profile, count = m(input)\n",
    "\n",
    "profile[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(677285.0625, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_metrics import ATACloss, profile_metrics\n",
    "\n",
    "criterion = ATACloss(weight_MSE=1)\n",
    "loss = criterion(tracks, profile, count)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0242868233369062,\n",
       " 1.0269171755998823,\n",
       " 1.0252296955673368,\n",
       " 1.0323565062471467,\n",
       " 1.0281575491595099,\n",
       " 1.0207284133491756,\n",
       " 1.0175975697749555,\n",
       " 1.0176447461398825,\n",
       " 1.035496942094676,\n",
       " 1.0181153806221328,\n",
       " 1.0151624622692277,\n",
       " 1.025698947062699,\n",
       " 1.0315933184393649,\n",
       " 1.0168293074189176,\n",
       " 1.0231442962397,\n",
       " 1.015189499130307,\n",
       " 1.0083193528481231,\n",
       " 1.0168074147803308,\n",
       " 1.0283298762745399,\n",
       " 1.012215962803128,\n",
       " 1.022288898699505,\n",
       " 1.0262645964559594,\n",
       " 1.0261720716508131,\n",
       " 1.0285287157616725,\n",
       " 1.018175952054657,\n",
       " 1.0145428424613363,\n",
       " 1.0245412981016622,\n",
       " 1.0221562695444164,\n",
       " 1.0237888410379068,\n",
       " 1.0275504210794684,\n",
       " 1.02138185085399,\n",
       " 1.0190031268863184]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_metrics(tracks, profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models import CATAC\n",
    "\n",
    "m = CATAC(nb_pred=15, nb_cell_type_CN=4, nb_conv=8)\n",
    "input = torch.randn(32, 4, 4096)\n",
    "print(input.shape)\n",
    "x, profile, count = m(input)\n",
    "\n",
    "profile[10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPNet uses a composite loss function consisting of a linear combination of a mean squared error (MSE) loss on the log of the total counts and a multinomial negative log likelihood loss (MNLL) for the profile probability output. We use a weight of [4.9, 4.3, 18.5, 9.8, 8.9, 4.8, 4.6, 4.9, 12.4, 15.4, 4.3, 6.3, 1.4, 2.6, 7.6, 2.3, 16.3, 7.1 & 3.7] for the MSE loss for clusters c0–c20 (c15-c16 combined as one model), and a weight of 1 for the MNLL loss in the linear combination. The MSE loss weight is derived as the median of total counts across all peak regions for each cluster divided by a factor of 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/Users/adorman/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/Users/adorman/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting deeplift\n",
      "  Downloading deeplift-0.6.13.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9 in /Users/adorman/.local/lib/python3.11/site-packages (from deeplift) (1.26.4)\n",
      "Building wheels for collected packages: deeplift\n",
      "  Building wheel for deeplift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deeplift: filename=deeplift-0.6.13.0-py3-none-any.whl size=36424 sha256=091fc6dcfd9d22cc479a2b139f6743120df92c7fe99c9b5127633c6a6a2ad196\n",
      "  Stored in directory: /Users/adorman/Library/Caches/pip/wheels/15/47/9c/66fd18175f190aa68311228c694ebc713548ae719974f1e25b\n",
      "Successfully built deeplift\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/Users/adorman/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: deeplift\n",
      "Successfully installed deeplift-0.6.13.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/Users/adorman/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/Users/adorman/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/Users/adorman/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adorman/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from data_processing.utils_data_preprocessing import one_hot_encode\n",
    "\n",
    "import shap\n",
    "\n",
    "def compute_shap_score(model ,seq, back):\n",
    "    \n",
    "    back = torch.reshape(back, (-1,4,len(seq)))\n",
    "    seq = torch.reshape(torch.from_numpy(seq), (-1,4,len(seq)))\n",
    "    explainer = shap.DeepExplainer(\n",
    "        model, back)\n",
    "    print(explainer.expected_value)\n",
    "    raw_scores = explainer.shap_values(seq, back)\n",
    "    \n",
    "    return raw_scores   \n",
    "\n",
    "def compute_importance_score(path_model, path_sequence, device):\n",
    "\n",
    "    #Load the model and sequenece to predict\n",
    "    model = torch.load(path_model)\n",
    "    model.to(device)\n",
    "    seq = pd.read_pickle(path_sequence).sequence.iloc[:100]\n",
    "\n",
    "    #On-hot encode the sequences\n",
    "    seq = seq.apply(lambda x: one_hot_encode(x))\n",
    "    \n",
    "    #Create shuffled sequences for background\n",
    "    background = [dinuc_shuffle(s, num_shufs=20) for s in seq]\n",
    "\n",
    "    #Compute importance score for each base of sequences\n",
    "    shap_scores = [compute_shap_score(model,s,torch.from_numpy(background[i])) for i,s in enumerate(seq)]\n",
    "\n",
    "    #Project the scores on the sequence\n",
    "    proj_score = seq\n",
    "    \n",
    "    return seq, shap_scores, proj_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BPNet\n",
    "model = BPNet()\n",
    "input = torch.randn(1, 4, 4096)\n",
    "\n",
    "torch.save(model, '../results/best_biasModel.pt')\n",
    "model(input).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5332989]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 0.24185432941698515 - Tolerance: 0.01",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m seq, shap_score, proj_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_importance_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../results/best_biasModel.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../results/background_GC_matched.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mcompute_importance_score\u001b[0;34m(path_model, path_sequence, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m background \u001b[38;5;241m=\u001b[39m [dinuc_shuffle(s, num_shufs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seq]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Compute importance score for each base of sequences\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m shap_scores \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mcompute_shap_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#Project the scores on the sequence\u001b[39;00m\n\u001b[1;32m     36\u001b[0m proj_score \u001b[38;5;241m=\u001b[39m seq\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m background \u001b[38;5;241m=\u001b[39m [dinuc_shuffle(s, num_shufs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seq]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Compute importance score for each base of sequences\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m shap_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mcompute_shap_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i,s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq)]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#Project the scores on the sequence\u001b[39;00m\n\u001b[1;32m     36\u001b[0m proj_score \u001b[38;5;241m=\u001b[39m seq\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mcompute_shap_score\u001b[0;34m(model, seq, back)\u001b[0m\n\u001b[1;32m     12\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(\n\u001b[1;32m     13\u001b[0m     model, back)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(explainer\u001b[38;5;241m.\u001b[39mexpected_value)\n\u001b[0;32m---> 15\u001b[0m raw_scores \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_scores\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/shap/explainers/_deep/__init__.py:135\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:214\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    212\u001b[0m             model_output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mX)\n\u001b[0;32m--> 214\u001b[0m     \u001b[43m_check_additivity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_phis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_phis, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# in this case we have multiple inputs and potentially multiple outputs\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_phis[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/LeKIra/lib/python3.11/site-packages/shap/explainers/_deep/deep_utils.py:20\u001b[0m, in \u001b[0;36m_check_additivity\u001b[0;34m(explainer, model_output_values, output_phis)\u001b[0m\n\u001b[1;32m     16\u001b[0m         diffs \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m output_phis[t][i]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, output_phis[t][i]\u001b[38;5;241m.\u001b[39mndim)))\n\u001b[1;32m     18\u001b[0m maxdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(diffs)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m maxdiff \u001b[38;5;241m<\u001b[39m TOLERANCE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe SHAP explanations do not sum up to the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output! This is either because of a \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     21\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounding error or because an operator in your computation graph was not fully supported. If \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     22\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe sum difference of \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m is significant compared to the scale of your model outputs, please post \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     23\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas a github issue, with a reproducible example so we can debug it. Used framework: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplainer\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Max. diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Tolerance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOLERANCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 0.24185432941698515 - Tolerance: 0.01"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seq, shap_score, proj_score = compute_importance_score( '../results/best_biasModel.pt',\n",
    "                                                       '../results/background_GC_matched.pkl', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeKIra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
